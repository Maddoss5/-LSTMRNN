{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d031e8d8-2594-4c50-9f6c-49b026af2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b97b951-a3cb-4bb6-8aad-750ce3fae8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"hello world this is retard number 1 . hello cunt you are a retard. hello stupid fuck you should kill yoruself . hello idiot you are so dumb no one wants to hire you . maybe you should stop being so dumb. You were born to kill yourself . You have nothing good going for you why are you still alive. Nobody likes you , you have no friends , no girl friends , you hate your job. why are you still alive . There is no point in you existing. So what are you gonna do? What are you gonna do about it ? This country hates you , people look at you with contempt , they want you gone . Why are you still here pajeet. I know this place is hell for me . I know no one likes me and most people hate me, I wished people were nice to me , i wish people would wouldnt look at me like i am a criminal. Its not my fault i was born brown. I have wanted to give up long ago , i wanted to go home ever since i came here. But i am no quitter. People who hate me for no reason i hope they dont have to face this upon themselves because that will make them realize how hard it is to be me.  Today i built Recurrent neural network by myself it feels pretty damn good. Maybe i should start building more and more ML models that would make me better at something .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6eeb867-1fd4-4be2-85c9-916072062cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"incel.txt\",\"r\" ,encoding='cp932', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7427195-7047-4925-b3b1-2aca675234ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a023e1a6-877f-4fce-ba0e-7d3f8b07dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a271942-9596-410e-ac8a-1af33d080927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5291 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#tokenizes based on bert-based\n",
    "tokens=tokenizer.tokenize(text) \n",
    "#converts tokens to integers\n",
    "input_ids=tokenizer.convert_tokens_to_ids(tokens)    \n",
    "#vocab size of tokenizer used \n",
    "vocab_size=tokenizer.vocab_size      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "998513b5-3bb8-477b-a46e-46378949c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=7\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "data=[]\n",
    "for i in range(len(input_ids)-seq_len):\n",
    "    dat_x=input_ids[i:i+seq_len]\n",
    "    dat_y=input_ids[i+seq_len]\n",
    "    data.append((torch.tensor(dat_x),dat_y))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4826c964-04d9-4ebb-8cde-64d2f36135e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=30, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "124b1150-01ab-4874-bf60-3687a9801bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11ce0807-7aa3-4af5-9c53-6724edf97329",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=32\n",
    "numberofheads=6\n",
    "numofencoderlayers=4\n",
    "ffnsize=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bbfb3a4-43d4-43e4-bde0-043641f1c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=embed_size, nhead=8, num_layers=6, dim_feedforward=30):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Define the transformer layer (encoder and decoder combined)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_layers, \n",
    "            num_decoder_layers=1,\n",
    "            dim_feedforward=dim_feedforward\n",
    "        )\n",
    "        \n",
    "        # Output layer to predict the next word\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Embed the input tokens\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer expects input in the shape (seq_len, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, seq_len, d_model) -> (seq_len, batch_size, d_model)\n",
    "        \n",
    "        # We assume that for simple language modeling, decoder input is same as encoder input\n",
    "        # Pass through transformer\n",
    "        memory = self.transformer(x, x)  # Both encoder and decoder inputs are the same\n",
    "        \n",
    "        # Take the last output token (corresponding to next token prediction)\n",
    "        x = memory[-1, :, :]\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca68dceb-dfd4-49c8-8787-482d031eff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maddo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(vocab_size=vocab_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d71bd3a4-1c01-49d8-9155-c6e21b78b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 8.547431253444003\n",
      "Epoch 11, Loss: 4.189363688398889\n",
      "Epoch 21, Loss: 2.4204785860864457\n",
      "Epoch 31, Loss: 1.6516662600350245\n",
      "Epoch 41, Loss: 1.1305913832564811\n",
      "Epoch 51, Loss: 0.8002841598064886\n",
      "Epoch 61, Loss: 0.6143423267087694\n",
      "Epoch 71, Loss: 0.4619989131898277\n",
      "Epoch 81, Loss: 0.3713481436052102\n",
      "Epoch 91, Loss: 0.34366658373109704\n",
      "Epoch 101, Loss: 0.2656532782577189\n",
      "Epoch 111, Loss: 0.2439288522806124\n",
      "Epoch 121, Loss: 0.23402800326798595\n",
      "Epoch 131, Loss: 0.19246547207720366\n",
      "Epoch 141, Loss: 0.1823111743334744\n",
      "Epoch 151, Loss: 0.1629829652354879\n",
      "Epoch 161, Loss: 0.17033872841186506\n",
      "Epoch 171, Loss: 0.15298474587448793\n",
      "Epoch 181, Loss: 0.1296267418955758\n",
      "Epoch 191, Loss: 0.13135914468748422\n",
      "Predicted next word: of\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.long()\n",
    "        targets = targets.long()\n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    if(epoch%10==0):\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Testing (Example)\n",
    "model.eval()\n",
    "test_input = torch.tensor([input_ids[:seq_len]])  # Test input sequence\n",
    "output = model(test_input)\n",
    "predicted_token = torch.argmax(output, dim=1).item()\n",
    "predicted_word = tokenizer.decode([predicted_token])\n",
    "print(f\"Predicted next word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234a947-dc7a-4b8c-ba99-cf63bec0e2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5553b93f-e16a-401f-b4fc-a704038dd9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how not, definitive on you know, it. i find is quite terrible, he speak, yes in some at craft a wrote : sex, he said : stu his ask tears, and busy, and going downtown, and travelling, and meeting a bunch of different people, \" he says. \" talking to cnn, bbc and the sun has made me realise that my regular daily life is very boring and monotonous. \" and it kind of made me realise that i can ' t really go back to sitting on a forum all day, talking about how miserable i am, and making podcasts about it. i have to make a change. \" even if you ' re just some random guy who lives with his mom, and makes videos about how you ' re a loser, you can still have some small impact on the world. so it just gave me confidence. \" after all the media attention, jack noticed that he was getting good feedback from unexpected sources. \" i was getting positive comments from\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_text, next_words):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(start_text)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    for _ in range(next_words):\n",
    "        #even though input_ids are less than sequence length here, it works because we arent using fixed dimensions neural network.\n",
    "        input_seq = torch.tensor(input_ids[-seq_len:], dtype=torch.long).unsqueeze(0)  # Ensure shape (1, seq_length)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)\n",
    "        next_token_id = output.argmax().item()\n",
    "        input_ids.append(next_token_id)\n",
    "    return tokenizer.decode(input_ids)\n",
    "\n",
    "\n",
    "print(generate_text(model, \" how \",200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5084a-bd55-409b-b7e8-1708d040600a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17728065-04bb-4fc1-8985-52fdca856ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
